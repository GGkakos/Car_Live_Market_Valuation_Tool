##### DIRECTORY TREE #####
==================================================
./
    temp_urls.json
    combine_files.py
    output.csv
    scrapy.cfg
    urls.json
    cargiant_data.json
    cargiant-scraper-3/
        middlewares.py
        settings.py
        pipelines.py
        items.py
        __init__.py
        runfile.py
        spiders/
            catalogue.py
            master2.py
            master.py
            ind_listing.py
            master3.py
            __init__.py
            --pycache--/
                master3.cpython-310.pyc
                cargiant_spider.cpython-313.pyc
                ind_listing.cpython-310.pyc
                cargiant_spider_new.cpython-313.pyc
                cargiant_spider_new.cpython-310.pyc
                master.cpython-310.pyc
                cargiant_spider.cpython-310.pyc
                catalogue.cpython-310.pyc
                __init__.cpython-310.pyc
                __init__.cpython-313.pyc
                master2.cpython-310.pyc
        --pycache--/
            middlewares.cpython-313.pyc
            settings.cpython-313.pyc
            __init__.cpython-310.pyc
            settings.cpython-310.pyc
            __init__.cpython-313.pyc
            middlewares.cpython-310.pyc

##### FILE: temp_urls.json
==================================================
["https://www.cargiant.co.uk/car/Volkswagen/Golf/KR68JFY", "https://www.cargiant.co.uk/car/Tesla/Model-X/LG67KKU", "https://www.cargiant.co.uk/car/Vauxhall/Astra/WP20CKK", "https://www.cargiant.co.uk/car/KIA/Stinger/LF19MVW", "https://www.cargiant.co.uk/car/Mini/Countryman/OW19YCZ", "https://www.cargiant.co.uk/car/Volkswagen/Golf/PN70KGZ", "https://www.cargiant.co.uk/car/Volvo/XC40/FM68EOS", "https://www.cargiant.co.uk/car/KIA/Rio/FV18DNO", "https://www.cargiant.co.uk/car/Volkswagen/Touareg/BL68AVG", "https://www.cargiant.co.uk/car/Seat/Arona/RV69XPB", "https://www.cargiant.co.uk/car/Volvo/XC40/LB19EMV", "https://www.cargiant.co.uk/car/Land-Rover/Range-Rover-Evoque/AK20EYR", "https://www.cargiant.co.uk/car/BMW/X2/DL67YMH", "https://www.cargiant.co.uk/car/Mercedes/S-Class/PJ69ZZR", "https://www.cargiant.co.uk/car/Volkswagen/Touareg/PL18PHA", "https://www.cargiant.co.uk/car/Mercedes/E-Class/PF19TBZ", "https://www.cargiant.co.uk/car/Mercedes/A-Class/YF20LKZ", "https://www.cargiant.co.uk/car/Ford/Edge/ET19VJJ", "https://www.cargiant.co.uk/car/Seat/Leon/BP70EYU", "https://www.cargiant.co.uk/car/Citroen/C1/LA21WYF", "https://www.cargiant.co.uk/car/Renault/Captur/EY20OHN", "https://www.cargiant.co.uk/car/Vauxhall/Crossland-X/NRZ9125", "https://www.cargiant.co.uk/car/KIA/Sportage/YF19FSB", "https://www.cargiant.co.uk/car/Seat/Arona/AJ21EWK", "https://www.cargiant.co.uk/car/Toyota/Corolla/WK20VWW", "https://www.cargiant.co.uk/car/Seat/Arona/MH21RKF", "https://www.cargiant.co.uk/car/KIA/Niro/LM72ATF"]

##### FILE: combine_files.py
==================================================
import os
from pathlib import Path
from datetime import datetime


def generate_tree(directory='.'):
    """Generate a tree-like directory structure."""
    tree_str = []
    for root, dirs, files in os.walk(directory):
        level = root.replace(directory, '').count(os.sep)
        indent = '    ' * level
        tree_str.append(f"{indent}{os.path.basename(root)}/")
        subindent = '    ' * (level + 1)
        for file in files:
            tree_str.append(f"{subindent}{file}")
    return "\n".join(tree_str)


def append_file_content_to_combined(file_path, output_file):
    """Append file content wrapped with headers to the combined file."""
    with open(output_file, 'a', encoding='utf-8') as combined:
        combined.write(f"##### FILE: {file_path}\n")
        combined.write("=" * 50 + "\n")
        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:
            combined.write(f.read())
        combined.write("\n\n")


def delete_previous_combined_files(output_file_prefix):
    """Delete all files starting with the output file prefix in the current directory."""
    for file in Path('.').glob(f"{output_file_prefix}*"):
        if file.is_file():
            file.unlink()


def main():
    extensions = [".py", ".json", ".cfg", ".txt", ".bat"]
    special_files = ["Dockerfile", ".dockerignore"]
    base_dir = '.'

    # Generate unique output filename with the current date and time
    now = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    output_file_prefix = "combined_code"
    output_file = f"{output_file_prefix}_{now}.txt"

    # Delete all previous combined_code files in the current directory
    delete_previous_combined_files(output_file_prefix)

    # Find target files to process
    files_to_append = []
    for root, _, files in os.walk(base_dir):
        for file in files:
            file_path = Path(root) / file
            if file_path.name.startswith(output_file_prefix):  # Skip output files
                continue
            if file_path.suffix in extensions or file_path.name in special_files:
                files_to_append.append(file_path)

    # Reject if more than 20 files to append
    if len(files_to_append) > 20:
        raise RuntimeError(
            f"Error: Too many files ({len(files_to_append)}) to process. Limit is 20.")

    # Add the directory tree at the beginning of the file
    tree_output = generate_tree(base_dir)
    with open(output_file, 'w', encoding='utf-8') as combined:
        combined.write("##### DIRECTORY TREE #####\n")
        combined.write("=" * 50 + "\n")
        combined.write(tree_output + "\n\n")

    # Append the selected files to the combined file
    for file_path in files_to_append:
        append_file_content_to_combined(file_path, output_file)

    print(f"Combined file created: {output_file}")


if __name__ == "__main__":
    main()


##### FILE: scrapy.cfg
==================================================
# Automatically created by: scrapy startproject
#
# For more information about the [deploy] section see:
# https://scrapyd.readthedocs.io/en/latest/deploy.html

[settings]
default = cargiant_scraper_3.settings

[deploy]
#url = http://localhost:6800/
project = cargiant_scraper_3


##### FILE: urls.json
==================================================
[
  "https://www.cargiant.co.uk/car/Volkswagen/Golf/KR68JFY",
  "https://www.cargiant.co.uk/car/Tesla/Model-X/LG67KKU",
  "https://www.cargiant.co.uk/car/Vauxhall/Astra/WP20CKK",
  "https://www.cargiant.co.uk/car/KIA/Stinger/LF19MVW",
  "https://www.cargiant.co.uk/car/Mini/Countryman/OW19YCZ",
  "https://www.cargiant.co.uk/car/Volkswagen/Golf/PN70KGZ",
  "https://www.cargiant.co.uk/car/Volvo/XC40/FM68EOS",
  "https://www.cargiant.co.uk/car/KIA/Rio/FV18DNO",
  "https://www.cargiant.co.uk/car/Volkswagen/Touareg/BL68AVG",
  "https://www.cargiant.co.uk/car/Seat/Arona/RV69XPB",
  "https://www.cargiant.co.uk/car/Volvo/XC40/LB19EMV",
  "https://www.cargiant.co.uk/car/Land-Rover/Range-Rover-Evoque/AK20EYR",
  "https://www.cargiant.co.uk/car/BMW/X2/DL67YMH",
  "https://www.cargiant.co.uk/car/Mercedes/S-Class/PJ69ZZR",
  "https://www.cargiant.co.uk/car/Volkswagen/Touareg/PL18PHA",
  "https://www.cargiant.co.uk/car/Mercedes/E-Class/PF19TBZ",
  "https://www.cargiant.co.uk/car/Mercedes/A-Class/YF20LKZ",
  "https://www.cargiant.co.uk/car/Ford/Edge/ET19VJJ",
  "https://www.cargiant.co.uk/car/Seat/Leon/BP70EYU",
  "https://www.cargiant.co.uk/car/Citroen/C1/LA21WYF",
  "https://www.cargiant.co.uk/car/Renault/Captur/EY20OHN",
  "https://www.cargiant.co.uk/car/Vauxhall/Crossland-X/NRZ9125",
  "https://www.cargiant.co.uk/car/KIA/Sportage/YF19FSB",
  "https://www.cargiant.co.uk/car/Seat/Arona/AJ21EWK",
  "https://www.cargiant.co.uk/car/Toyota/Corolla/WK20VWW",
  "https://www.cargiant.co.uk/car/Seat/Arona/MH21RKF",
  "https://www.cargiant.co.uk/car/KIA/Niro/LM72ATF"
]

##### FILE: cargiant_data.json
==================================================
[
{"url": "https://www.cargiant.co.uk/car/Tesla/Model-X/LG67KKU", "brand": "Tesla", "model": "Model X", "Price": "26799", "Year": "2017", "Mileage": "57,467", "Fuel": "Electric", "Transmission": "Auto", "Body": "SUV", "litres": "0.001", "hp": "328"},
{"url": "https://www.cargiant.co.uk/car/Seat/Arona/MH21RKF", "brand": "Seat", "model": "Arona", "Price": "12499", "Year": "2021", "Mileage": "40,334", "Fuel": "Petrol", "Transmission": "Manual", "Body": "SUV", "litres": "0.999", "hp": "110"},
{"url": "https://www.cargiant.co.uk/car/Volkswagen/Golf/GK71HVY", "brand": "Volkswagen", "model": "Golf", "Price": "14299", "Year": "2021", "Mileage": "45,057", "Fuel": "Petrol", "Transmission": "Manual", "Body": "5 Dr Hatch", "litres": "1.498", "hp": "150"},
{"url": "https://www.cargiant.co.uk/car/Skoda/Fabia/NC19DJZ", "brand": "Skoda", "model": "Fabia", "Price": "8799", "Year": "2019", "Mileage": "53,406", "Fuel": "Petrol", "Transmission": "Manual", "Body": "5 Dr Hatch", "litres": "0.999", "hp": "110"},
{"url": "https://www.cargiant.co.uk/car/Mini/Electric/HV71UUP", "brand": "Mini", "model": "Electric", "Price": "13599", "Year": "2021", "Mileage": "7,800", "Fuel": "Electric", "Transmission": "Auto", "Body": "3 Dr Hatch", "litres": "0.001", "hp": "184"}
]

##### FILE: cargiant-scraper-3/middlewares.py
==================================================
import logging
from selenium.webdriver.remote.remote_connection import LOGGER as selenium_logger
from scrapy.http import HtmlResponse
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from scrapy import signals

# Suppress unnecessary Selenium logs
selenium_logger.setLevel(logging.WARNING)

class SeleniumMiddleware:
    def __init__(self):
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # Run in headless mode
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")  # Required for some environments
        chrome_options.add_argument("--disable-dev-shm-usage")  # Overcome limited resource problems
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)

    @classmethod
    def from_crawler(cls, crawler):
        middleware = cls()
        crawler.signals.connect(middleware.spider_closed, signal=signals.spider_closed)
        return middleware

    def process_request(self, request, spider):
        logging.info(f"Processing URL: {request.url}")
        self.driver.get(request.url)

        try:
            # Wait until the target element is present
            WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div[data-vehicle]"))
            )
        except Exception as e:
            logging.error(f"Error loading page {request.url}: {e}")
            return HtmlResponse(
                url=self.driver.current_url,
                status=500,
                request=request,
                body=f"Error loading page: {e}".encode('utf-8')
            )

        body = self.driver.page_source
        return HtmlResponse(
            url=self.driver.current_url,
            body=body,
            encoding='utf-8',
            request=request
        )

    def spider_closed(self, spider):
        logging.info("Closing Selenium WebDriver.")
        self.driver.quit()


##### FILE: cargiant-scraper-3/settings.py
==================================================
# Scrapy settings for cargiant_scraper_3 project

BOT_NAME = "cargiant_scraper_3"

SPIDER_MODULES = ["cargiant_scraper_3.spiders"]
NEWSPIDER_MODULE = "cargiant_scraper_3.spiders"

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# Enable or disable downloader middlewares
DOWNLOADER_MIDDLEWARES = {
    # 'cargiant_scraper_3.middlewares.SeleniumMiddleware': 543,
    'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 810,
}

# Enable and configure HTTP caching (disabled by default)
HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'

# Set settings whose default value is deprecated to a future-proof value
REQUEST_FINGERPRINTER_IMPLEMENTATION = "2.7"
TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"
FEED_EXPORT_ENCODING = "utf-8"

LOG_LEVEL = 'INFO'  # Set this to 'ERROR' if you only want to see errors.


##### FILE: cargiant-scraper-3/pipelines.py
==================================================
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter


class CargiantScraperPipeline:
    def process_item(self, item, spider):
        return item


##### FILE: cargiant-scraper-3/items.py
==================================================
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class CargiantScraperItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass


##### FILE: cargiant-scraper-3/__init__.py
==================================================


##### FILE: cargiant-scraper-3/runfile.py
==================================================
import os
from scrapy import cmdline

# Ensure we are in the correct directory before running the Scrapy command
# target_dir = './Marij/cargiant_scraper_3'
# current_dir = os.getcwd()

# if current_dir != os.path.abspath(target_dir):
#     os.chdir(target_dir)

# Execute the Scrapy command
cmdline.execute("scrapy crawl master3 -O cargiant_data.json".split())


##### FILE: cargiant-scraper-3/spiders/catalogue.py
==================================================
import scrapy
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from scrapy.http import HtmlResponse
import logging
import time

class CargiantSpider(scrapy.Spider):
    name = "catalogue"
    start_urls = ['https://www.cargiant.co.uk/search/all/all']

    def __init__(self, *args, **kwargs):
        super(CargiantSpider, self).__init__(*args, **kwargs)
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # Run in headless mode
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--no-sandbox')
        self.driver = webdriver.Chrome(options=chrome_options)
    
    def parse(self, response):
        self.driver.get(response.url)

        for page_num in range(5):  # change number of pages
            self.logger.info(f"Processing page {page_num + 1}")
            
            # Wait for the listings to load
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-vehicle]'))
                )
            except Exception as e:
                self.logger.error(f"Error loading listings: {e}")
                break

            # Grab the current HTML source
            body = self.driver.page_source
            response_obj = HtmlResponse(
                url=self.driver.current_url,
                body=body,
                encoding='utf-8',
            )

            # Parse car listing links
            listings = response_obj.css('a.car-listing-item__details')
            if not listings:
                self.logger.warning("No listings found!")

            for listing in listings:
                car_url = listing.attrib.get('href')
                if car_url:
                    # Construct the absolute URL
                    full_url = f"https://www.cargiant.co.uk{car_url}"
                    yield {
                        'url': full_url,
                    }
                else:
                    self.logger.warning("No URL found in a listing.")
            
            # Handle pagination by clicking the "Next" button
            try:
                next_button = WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'a.paging__item--next'))
                )
                self.driver.execute_script("arguments[0].click();", next_button)
                time.sleep(2)  # Allow time for the next page to load
            except Exception as e:
                self.logger.error(f"Error clicking next button: {e}")
                break

    def closed(self, reason):
        self.driver.quit()


##### FILE: cargiant-scraper-3/spiders/master2.py
==================================================
import scrapy
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from scrapy.http import HtmlResponse
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import time
import logging


class MasterSpider(scrapy.Spider):
    name = "master2"
    start_urls = ['https://www.cargiant.co.uk/search/all/all']

    def __init__(self, *args, **kwargs):
        super(MasterSpider, self).__init__(*args, **kwargs)
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # Run in headless mode
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--no-sandbox')
        self.driver = webdriver.Chrome(options=chrome_options)

    def parse(self, response):
        self.driver.get(response.url)

        for page_num in range(5):  # Change this number to the desired number of pages
            self.logger.info(f"Processing page {page_num + 1}")

            # Wait for the listings to load
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located(
                        (By.CSS_SELECTOR, 'div[data-vehicle]'))
                )
            except Exception as e:
                self.logger.error(f"Error loading listings: {e}")
                break

            # Grab the current HTML source
            body = self.driver.page_source
            response_obj = HtmlResponse(
                url=self.driver.current_url,
                body=body,
                encoding='utf-8',
            )

            # Parse car listing links
            listings = response_obj.css('a.car-listing-item__details')
            if not listings:
                self.logger.warning("No listings found!")

            for listing in listings[:1]:
                car_url = listing.attrib.get('href')
                if car_url:
                    # Construct the absolute URL
                    full_url = f"https://www.cargiant.co.uk{car_url}"
                    yield scrapy.Request(url=full_url, callback=self.parse_listing)
                else:
                    self.logger.warning("No URL found in a listing.")

            # Handle pagination by clicking the "Next" button
            try:
                next_button = WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located(
                        (By.CSS_SELECTOR, 'a.paging__item--next'))
                )
                self.driver.execute_script(
                    "arguments[0].click();", next_button)
                time.sleep(2)  # Allow time for the next page to load
            except Exception as e:
                self.logger.error(f"Error clicking next button: {e}")
                break

    def parse_listing(self, response):
        self.logger.info(f"Processing next listing ...")
        self.driver.get(response.url)
        time.sleep(0.2)  # Wait for the page to load

        # Initialize dictionary for output
        output = {}
        output["url"] = response.url

        # Extract the title which includes both brand and model
        try:
            title_element = self.driver.find_element(
                By.CSS_SELECTOR, 'h1.title__main.set-h3')
            title = title_element.text.strip()
            title_parts = title.split(None, 1)  # Split into brand and model
            output["brand"] = title_parts[0]
            output["model"] = title_parts[1] if len(title_parts) > 1 else None
        except Exception as e:
            self.logger.error(f"Error extracting title: {e}")
            output["brand"] = None
            output["model"] = None

        # Extract price from top part
        try:
            price_element = self.driver.find_element(
                By.CSS_SELECTOR, 'div.price-block__price')
            price = price_element.text.strip()
            output["Price"] = price.replace('£', '').replace(',', '').strip()
        except Exception as e:
            self.logger.error(f"Error extracting price: {e}")
            output["Price"] = None

        # Collect all items from details section on page
        details = {}
        try:
            items = self.driver.find_elements(
                By.CSS_SELECTOR, 'li.details-panel-item__list__item')
            for item in items:
                spans = item.find_elements(By.CSS_SELECTOR, 'span')
                if len(spans) >= 2:
                    key = spans[0].text.strip()
                    value = spans[1].text.strip()
                    details[key] = value
        except Exception as e:
            self.logger.error(f"Error extracting details: {e}")

        # Extract the required metrics from the details dictionary
        output["Year"] = details.get('Year')
        output["Mileage"] = details.get('Mileage')
        output["Fuel"] = details.get('Fuel Type')
        output["Transmission"] = details.get('Transmission')
        output["Body"] = details.get('Body Type')

        # Click on the Performance tab to extract additional data
        try:
            # Wait until the Performance tab is clickable and click it
            performance_tab = WebDriverWait(self.driver, 10).until(
                EC.element_to_be_clickable(
                    (By.CSS_SELECTOR, 'div.tab-wrap__head__inner__tabs__tab[data-tab="tab1"]'))
            )
            performance_tab.click()
            time.sleep(0.2)  # Wait for the Performance tab to load

            # Extract CC and Engine Power BHP from the Performance tab
            # Locate all rows in the table
            rows = self.driver.find_elements(By.CSS_SELECTOR, 'tbody tr')
            cc = None
            bhp = None

            # Iterate through rows and extract data
            for row in rows:
                try:
                    # Check for presence of key/value elements
                    key_element = row.find_elements(By.CSS_SELECTOR, 'td.key')
                    value_element = row.find_elements(
                        By.CSS_SELECTOR, 'td.value')

                    if key_element and value_element:  # Ensure both elements exist
                        key = key_element[0].text.strip()
                        value = value_element[0].text.strip()

                        # Check for desired keys
                        if key == 'CC':
                            cc = value
                        elif key == 'Engine Power - BHP':
                            bhp = value
                except Exception as e:
                    self.logger.error(f"Error parsing row: {e}")
                    continue

            # Convert CC to litres if available
            if cc:
                try:
                    output["litres"] = str(
                        float(cc.replace(',', '').strip()) / 1000) if cc else None
                except ValueError:
                    output["litres"] = None
            else:
                output["litres"] = None

            # Store BHP
            output["hp"] = bhp if bhp else None

        except Exception as e:
            self.logger.error(f"Error extracting Performance data: {e}")
            output["litres"] = None
            output["hp"] = None

        # Yield the output as an item
        yield output

    def closed(self, reason):
        self.driver.quit()


##### FILE: cargiant-scraper-3/spiders/master.py
==================================================
import scrapy
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from scrapy.http import HtmlResponse
import json
import os
import time
import subprocess
import sys

class CargiantSpider(scrapy.Spider):
    name = "master"
    start_urls = ['https://www.cargiant.co.uk/search/all/all']

    def __init__(self, *args, **kwargs):
        super(CargiantSpider, self).__init__(*args, **kwargs)
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # Run in headless mode
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--no-sandbox')
        self.driver = webdriver.Chrome(options=chrome_options)
        self.urls_file = "urls.json"
        self.results_file = "results.json"

        # Delete old files if they exist
        if os.path.exists(self.urls_file):
            os.remove(self.urls_file)
        if os.path.exists(self.results_file):
            os.remove(self.results_file)

    def parse(self, response):
        self.driver.get(response.url)

        all_urls = []  # Save all the car listing URLs

        for page_num in range(1):  # Adjust number of pages as needed
            self.logger.info(f"Processing page {page_num + 1}")

            # Wait for the listings to load
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-vehicle]'))
                )
            except Exception as e:
                self.logger.error(f"Error loading listings: {e}")
                break

            # Grab the current HTML source
            body = self.driver.page_source
            response_obj = HtmlResponse(
                url=self.driver.current_url,
                body=body,
                encoding='utf-8',
            )

            # Parse car listing links
            listings = response_obj.css('a.car-listing-item__details')
            if not listings:
                self.logger.warning("No listings found!")

            for listing in listings:
                car_url = listing.attrib.get('href')
                if car_url:
                    # Construct the absolute URL
                    full_url = f"https://www.cargiant.co.uk{car_url}"
                    all_urls.append(full_url)
                else:
                    self.logger.warning("No URL found in a listing.")

            # Handle pagination by clicking the "Next" button
            try:
                next_button = WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, 'a.paging__item--next'))
                )
                self.driver.execute_script("arguments[0].click();", next_button)
                time.sleep(2)  # Allow time for the next page to load
            except Exception as e:
                self.logger.error(f"Error clicking next button: {e}")
                break

        # Save all URLS to urls.json
        self.logger.info(f"Found {len(all_urls)} URLs in total.")
        self.save_urls_to_json(all_urls)

        # Call the individual listing spider for each URL
        self.crawl_ind_listing(all_urls)

    def save_urls_to_json(self, urls):
        """Save all collected URLs to a JSON file."""
        try:
            with open(self.urls_file, 'w') as f:
                json.dump(urls, f, indent=2)
            self.logger.info(f"Saved {len(urls)} URLs to {self.urls_file}.")
        except Exception as e:
            self.logger.error(f"Error saving URLs to file: {e}")

    def crawl_ind_listing(self, urls):
        """Run the 'ind_listing' spider once with all URLs."""
        # Write all URLs to a temporary file
        temp_urls_file = 'temp_urls.json'
        with open(temp_urls_file, 'w') as f:
            json.dump(urls, f)

        try:
            # Get the project root directory (where scrapy.cfg is located)
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            self.logger.info(f"Using project root directory: {project_root}")

            # Update PYTHONPATH in the environment variables
            env = os.environ.copy()
            env['PYTHONPATH'] = project_root + os.pathsep + env.get('PYTHONPATH', '')

            # Ensure proper directory context
            os.chdir(project_root)

            # Use sys.executable to ensure the subprocess uses the same Python environment
            subprocess.run(
                [
                    sys.executable, "-m", "scrapy", "crawl", "ind_listing",
                    "-a", f"urls_file={temp_urls_file}",
                    "-o", self.results_file,
                    "-t", "json"
                ],
                check=True,
                env=env  # Pass the updated environment variables
            )
        except subprocess.CalledProcessError as e:
            self.logger.error(f"Error while running ind_listing spider: {e}")

    def closed(self, reason):
        self.driver.quit()


##### FILE: cargiant-scraper-3/spiders/ind_listing.py
==================================================
import scrapy
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import json
import logging
import time

class CargiantSpider(scrapy.Spider):
    name = 'ind_listing'

    def __init__(self, *args, **kwargs):
        super(CargiantSpider, self).__init__(*args, **kwargs)
        chrome_options = Options()
        chrome_options.add_argument('--headless')  # Run in headless mode
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--no-sandbox')
        self.driver = webdriver.Chrome(options=chrome_options)

        # Read URLs from the passed file
        urls_file = kwargs.get('urls_file')
        if urls_file:
            self.logger.info(f"Reading URLs from file: {urls_file}")
            with open(urls_file, 'r') as f:
                self.start_urls = json.load(f)
            self.logger.info(f"Loaded {len(self.start_urls)} URLs.")
        else:
            self.logger.error("No URLs file provided!")
            self.start_urls = []

    def start_requests(self):
        # Log that start_requests is invoked
        self.logger.info("Starting to process URLs...")
        for url in self.start_urls:
            self.logger.info(f"Processing URL: {url}")
            yield scrapy.Request(url, self.parse)

    def parse(self, response):
        self.driver.get(response.url)
        time.sleep(1)  # Wait for the page to load

        # Initialize dictionary for output
        output = {"url": response.url}

        # Extract title
        try:
            title_element = self.driver.find_element(By.CSS_SELECTOR, 'h1.title__main.set-h3')
            title = title_element.text.strip()
            title_parts = title.split(None, 1)  # Split into brand and model
            output["brand"] = title_parts[0]
            output["model"] = title_parts[1] if len(title_parts) > 1 else None
        except Exception as e:
            self.logger.error(f"Error extracting title: {e}")
            output["brand"] = None
            output["model"] = None

        # Log the parsed output
        self.logger.info(f"Scraped data: {output}")

        yield output

    def closed(self, reason):
        self.driver.quit()


##### FILE: cargiant-scraper-3/spiders/master3.py
==================================================
import scrapy
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from scrapy.http import HtmlResponse
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
import logging
from scrapy.exceptions import CloseSpider


class MasterSpider(scrapy.Spider):
    name = "master3"
    start_urls = ["https://www.cargiant.co.uk/search/all/all"]
    custom_settings = {
        "CONCURRENT_REQUESTS": 16,
        "RETRY_ENABLED": True,
    }

    def __init__(self, *args, **kwargs):
        super(MasterSpider, self).__init__(*args, **kwargs)
        self.collected_urls = []  # Collect all URLs here in phase 1
        # self.logger = logging.getLogger(__name__)

        # Selenium WebDriver setup
        chrome_options = Options()
        chrome_options.add_argument("--headless")  # Run browser in headless mode
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--no-sandbox")
        self.driver = webdriver.Chrome(options=chrome_options)

    def parse(self, response):
        """
        Phase 1: Use Selenium to collect all listing URLs from pagination.
        """
        self.logger.info("Starting phase 1: Collecting listing URLs.")
        self.driver.get(response.url)

        while True:
            # Wait for the listings to load
            try:
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located(
                        (By.CSS_SELECTOR, "div[data-vehicle]")
                    )
                )
                self.logger.info(f"Page loaded: {self.driver.current_url}")
            except TimeoutException:
                self.logger.error("Timeout while waiting for listings. Stopping.")
                break

            # Grab the current HTML and extract listings
            body = self.driver.page_source
            current_response = HtmlResponse(
                url=self.driver.current_url,
                body=body,
                encoding="utf-8",
            )
            listings = current_response.css(
                "a.car-listing-item__details::attr(href)"
            ).extract()
            absolute_urls = [f"https://www.cargiant.co.uk{url}" for url in listings]
            self.collected_urls.extend(absolute_urls)
            self.logger.info(f"Collected {len(absolute_urls)} URLs from this page.")

            # Try to navigate to the next page
            try:
                next_button = WebDriverWait(self.driver, 10).until(
                    EC.element_to_be_clickable(
                        (By.CSS_SELECTOR, "a.paging__item--next")
                    )
                )
                self.driver.execute_script("arguments[0].click();", next_button)
            except TimeoutException:
                self.logger.info("No more pages to navigate. Exiting phase 1.")
                break

        # After collecting all URLs, move to phase 2
        if not self.collected_urls:
            self.logger.error("No URLs found during phase 1.")
            raise CloseSpider("No listings to scrape.")

        self.logger.info(
            f"Collected a total of {len(self.collected_urls)} listing URLs."
        )
        for url in self.collected_urls:
            yield scrapy.Request(url=url, callback=self.parse_listing)

        # Close the driver after Phase 1
        self.driver.quit()

    def parse_listing(self, response):
        """
        Phase 2: Use Scrapy to parse individual listing pages for details.
        """
        self.logger.info(f"Scraping details from {response.url}")

        output = {}
        output["url"] = response.url

        # Extract the title (brand and model)
        title = response.css("h1.title__main.set-h3::text").get()
        if title:
            title_parts = title.strip().split(None, 1)
            output["brand"] = title_parts[0]
            output["model"] = title_parts[1] if len(title_parts) > 1 else None
        else:
            output["brand"] = None
            output["model"] = None

        # Extract price
        price = response.css("div.price-block__price::text").get()
        output["Price"] = (
            price.replace("£", "").replace(",", "").strip() if price else None
        )

        # Extract details section
        details = {}
        for item in response.css("li.details-panel-item__list__item"):
            key = item.css("span:nth-child(1)::text").get()
            value = item.css("span:nth-child(2)::text").get()
            if key and value:
                details[key.strip()] = value.strip()

        output["Year"] = details.get("Year")
        output["Mileage"] = details.get("Mileage")
        output["Fuel"] = details.get("Fuel Type")
        output["Transmission"] = details.get("Transmission")
        output["Body"] = details.get("Body Type")

        # Extract Performance tab data
        cc = response.xpath("//th[text()='CC']/following-sibling::td/text()").get()
        bhp = response.xpath(
            "//th[text()='Engine Power - BHP']/following-sibling::td/text()"
        ).get()

        # Convert CC to litres if available
        if cc:
            try:
                output["litres"] = (
                    str(float(cc.replace(",", "").strip()) / 1000) if cc else None
                )
            except ValueError:
                output["litres"] = None
        else:
            output["litres"] = None

        # Store BHP
        output["hp"] = bhp if bhp else None

        yield output


##### FILE: cargiant-scraper-3/spiders/__init__.py
==================================================
# This package will contain the spiders of your Scrapy project
#
# Please refer to the documentation for information on how to create and manage
# your spiders.


